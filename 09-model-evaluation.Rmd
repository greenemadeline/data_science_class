---
title: "Introduction to model evaluation"
subtitle: "Data Science for Biologists, Spring 2020"
author: "Madeline Greene"
output: 
  html_document:
    highlight: tango
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom) 
library(modelr)
library(patchwork)
set.seed(24)
```

## Instructions

Standard grading criteria apply, except there is no "answer style" - just write out answers normally! **Make sure your bulleted lists render appropriately in the knitted output!!!**

This assignment will use an external dataset of various physical measurements from 250 adult males. Our goal for this assignment is to build and evaluate a model from this data to **predict body fat percentage** (column `Percent`) in adult males, and then use this model to predict future outcomes. Age is measured in years, weight in pounds, height in inches, and all other measurements are circumference measured in cm.

```{r, collapse=T}
fatmen <- read_csv("https://raw.githubusercontent.com/sjspielman/datascience_for_biologists/master/data/bodyfat.csv")
dplyr::glimpse(fatmen)
```



## Part 1: Build a model using AIC stepwise model selection

Using the `step()` function, determine the most appropriate model to explain variation in bodyfat percentage in this data. Examine the model output with the `summary` function, and answer questions below. **You will use this model (aka you will specify these predictors) for all model evaluation questions.**

```{r}
## Use step() to build and save a model to explain Percent. PLEASE use the argument trace=F when calling step()!!

step(lm(Percent ~ . , data = fatmen), trace = F) -> final_model

## Examine output with summary OR broom functions tidy and glance

broom::tidy(final_model)
broom::glance(final_model)
```

#### Part 1 questions: Answer the questions in the templated bullets!

1. In a bulleted list below, state the predictor variables for the final model and their P-values. You do not need to worry about coefficients!!

    + Age, P = 3.03e-2    
    + Weight, P = 6.14e-4
    + Neck, P = 9.77e-2
    + Abdomen, P = 2.22e-29
    + Thigh, P = 6.13e-2
    + Forearm, P = 3.17e-3
    + Wrist, P = 2.88e-3

2. What percentage of variation in bodyfat percentage is explained by this model? 

    + 74.2%


3. What percentage of variation in bodyfat percentage is UNEXPLAINED by this model?
  
    + 25.8%

4. What is the RMSE of your model? Hint: you need to run some code!

    ```{r}
    ## code to get RMSE of model, using the function modelr::rmse()
    
rmse(final_model, fatmen)
    ```
  
    + 4.231338


## Part 2: Evaluate the model using several approaches

### Part 2.1: Training and testing approach

**First, use a simple train/test approach**, where the training data is a random subset comprising 65% of the total dataset. Determine the R-squared (`modelr::rsquare()`) and RMSE (`modelr::rmse()`)  as determined from the training AND testing data.

```{r}

percent_formula <- as.formula("Percent ~ Age + Weight + Neck + Abdomen + Thigh + Forearm + Wrist") 

## split data into train and test, using this variable as part of your code:
training_frac <- 0.65

training_data <- dplyr::sample_frac(fatmen, training_frac)
testing_data <- dplyr::anti_join(fatmen, training_data)


## Train model on training data. DO NOT USE summary(), just fit the model with the training data.

trained_model <- lm(percent_formula, data = training_data)

## Determine metrics on TRAINING data (R-squared and RMSE), using the trained model

rsquare(trained_model, training_data)
rmse(trained_model, training_data)

## Determine metrics on TESTING data (R-squared and RMSE), using the trained model

rsquare(trained_model, testing_data)
rmse(trained_model, testing_data)
```

#### Part 2.1 questions: Answer the questions in the templated bullets!

1. Compare the training data $R^2$ to the testing data $R^2$. Which is higher (i.e., does the model run on training or testing data explain more variation in Percent), and is this outcome expected?

  + Training R^2 = 0.7201342 but testing R^2 = 0.7600382 Testing is higher. This is an unexpected outcome. 

2. Compare the training data *RMSE* to the testing data *RMSE*. Which is *lower* (i.e., is there more error from the model run on training or testing data), and is this outcome expected?

  + Training RMSE = 4.326385 but testing RMSE = 4.235059 Testing is lower. This is an unexpected outcome. 




### Part 2.2: K-fold cross validation

Use k-fold cross validation with **15 folds** to evaluate the model. Determine the $R^2$ and RMSE for each fold, and *visualize* the distributions of $R^2$ and RMSE in two separate plots that you *add together with patchwork*. You should also calculate the mean $R^2$ and mean RMSE values.

```{r}
## First define the FUNCTION you will use with purrr::map which contains your linear model.
## Do NOT use step() in here - you should have used step in Part 1 to know which predictors should be included here
my_bodyfat_model <- function(input_data){
  lm(percent_formula, data = input_data)  
}

## perform k-fold cross validation, using this variable in your code
number_folds <- 15
crossv_kfold(fatmen, number_folds) %>%
  mutate(model = purrr::map(train, my_bodyfat_model), 
         rsquared = purrr::map2_dbl(model, test , rsquare), 
         rmse_value = purrr::map2_dbl(model, test , rmse)) -> final_kfold

## Calculate the mean R^2 and RMSE 

mean(final_kfold$rsquared)
mean(final_kfold$rmse_value)
    

## Make figures for R^2 and RMSE, which clearly show the MEAN values for each distribution using stat_summary() or similar (unless you make a boxplot, which already shows the median)

final_kfold %>%
  #rmse values
  ggplot(aes(y = rmse_value)) +
  #geom_boxplot allows for summary
  geom_boxplot() +
  #label axis
  labs(x = "Training",
       y = "RMSE Mean")

final_kfold %>%
  #rsquare values
  ggplot(aes(y = rsquared)) +
  #geom_boxplot allows for summary
  geom_boxplot() +
  #label axis
  labs(x = "Training",
       y = "R^2 Mean")

```

#### Part 2.2 questions: Answer the questions in the templated bullets!

1. Examine your distribution of $R^2$ values. What is the average $R^2$, and how does it compare to the **testing $R^2$** from Part 1?

    + The average $R^2$ is 0.6952857. It is lower than the testing $R^2$ from part 1 which is 0.760038. 

2. Examine your distribution of *RMSE* values. What is the average *RMSE*, and how does it compare to the **testing RMSE** from Part 1?

    + The average RMSE is 4.343163. It is higher than the testing RMSE from part 1 which is 4.235059. 
  


### Part 2.3: Leave-one-out cross validation (LOOCV)

```{r}
## perform LOOCV (using the function my_bodyfat_model defined in Part 2.2)

crossv_loo(fatmen) %>%
  mutate(model = purrr::map(train, my_bodyfat_model),
         rmse_value = purrr::map2_dbl(model, test , rmse)) -> final_loo

## Calculate the mean RMSE 

mean(final_loo$rmse_value)


## Make figure of RMSE distribution, which clearly shows the MEAN value for the distribution using stat_summary() (unless you make a boxplot, which already shows the median)

final_loo %>%
  #RMSE value
  ggplot(aes(y = rmse_value)) +
  #geom_boxplot allows for summary
  geom_boxplot() +
  #label y axis
  labs(y = "Mean RMSE")

```

#### Part 2.3 question: Answer the questions in the templated bullets!

1. Examine your distribution of *RMSE* values. What is the average *RMSE*, and how does it compare to the **testing RMSE** from Part 1? How does it compare to the average *RMSE* from k-fold cross validation?

    + The average RMSE is 3.58246. The testing RMSE from part 1 is 4.235059. The RMSE from the k-fold cross validation is 4.343163. The average RMSE is lower than both the testing RMSE from part 1 and from the k-fold cross validation. 

### Part 2.4: Wrap-up

Considering all three approaches, do you believe this model is highly explanatory of Percent (e.g., how are the $R^2$ values)? Further, do you believe the error in this model is low, moderate or high (e.g., how are the RMSE values)? Answer in 1-2 sentences in the bullet:

  + The $R^2$ values averaged around 70%, which is a high percentage compared to 50%, but the $R^2$ value could be much higher, such around 95%. Because of the calculated $R^2$ values, I do not believe that this model is highly explanatory in making future predictions. The error in this model is low because the RMSE value is low, which means that there is a minimal spread of the data points, so it is concentrated around the best fit. 
  

## Part 3: Predictions

New men have arrived, and we want to use our model to predict their body fat percentages! Using the function `modelr::add_predictions()` use our model to predict what the body fat percentages will be for three men with the following physical attributes.

+ Bob
  + 37 years of Age
  + Weight of 195 pounds
  + 43.6 cm Neck circumference
  + 110.6 cm Abdomen circumference
  + 71.7 cm Thigh circumference
  + 31.2 Forearm circumference
  + 19.2 Wrist circumference
+ Bill
  + 65 years of Age
  + Weight of 183 pounds
  + 41.2 cm Neck circumference
  + 90.1 cm Abdomen circumference
  + 77.5 cm Thigh circumference
  + 32.2 cm Forearm circumference
  + 18.2 cm Wrist circumference
+ Fred
  + 19 years of Age
  + Weight of 121 pounds
  + 30.2 cm Neck circumference
  + 68 cm Abdomen circumference
  + 48.1 cm Thigh circumference
  + 23.8 cm Forearm circumference
  + 16.1 cm Wrist circumference

```{r}
## Make a SINGLE tibble with THREE ROWS (one per observed new man), and use this tibble to predict outcomes with `modelr::add_predictions()

tibble(Age = c(37, 65, 19),
       Weight = c(195, 183,121),
       Neck = c(43.6, 41.2, 30.2),
       Abdomen = c(110.6, 90.1, 68),
       Thigh = c(71.7, 77.5, 48.1), 
       Forearm = c(31.2, 32.2, 23.8),
       Wrist = c(19.2, 18.2, 16.1)) -> new_men

new_men
       
modelr::add_predictions(new_men, final_model)


## HINT: See the tidyr assignment for different ways to make a tibble directly within R

```

#### Part 3 answers:

Stick the answer after the colon for each bullet **in bold**:

+ Bob's predicted body fat percent is: **33.6%**
+ Bill's predicted body fat percent is: **22.6%**
+ Fred's predicted body fat percent is: **2.93%**


**BONUS QUESTION!**
Which of the three predictions (Bob, Bill, and Fred) do you think is LEAST reliable? You may need some code to figure out which one, so add in below as needed!!

```{r}
ggplot(fatmen, aes( x = Age, y = Percent)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  #vline of each name 
  geom_vline(xintercept = 37, col = "plum3") +  
  geom_vline(xintercept = 65, col = "salmon") +
  geom_vline(xintercept = 19, col = "turquoise3") +
  #name each vline
  annotate("text", x = 22, y = 40, label = "Fred") +
  annotate("text", x = 39, y = 45, label = "Bob") +
  annotate("text", x = 67, y = 42, label = "Bill")
  

#Fred is probably most unrealiable 
#summary(fatmen) 
#freds numbers are farther away 

```
